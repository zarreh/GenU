{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse, urlunparse\n",
    "\n",
    "\n",
    "def is_allowed_domain(link, main_domain, additional_domains):\n",
    "    \"\"\"\n",
    "    Check if the link belongs to the main domain or its subdomains,\n",
    "    or any of the additional allowed domains.\n",
    "    \"\"\"\n",
    "    parsed_link = urlparse(link)\n",
    "    domain = parsed_link.netloc\n",
    "\n",
    "    # Allow the main domain and all its subdomains\n",
    "    if domain == main_domain or domain.endswith(f\".{main_domain}\"):\n",
    "        return True\n",
    "\n",
    "    # Allow explicitly allowed additional domains\n",
    "    return any(\n",
    "        domain == add_domain or domain.endswith(f\".{add_domain}\")\n",
    "        for add_domain in additional_domains\n",
    "    )\n",
    "\n",
    "\n",
    "def is_pdf_file(link):\n",
    "    \"\"\"Check if the link points to a PDF file.\"\"\"\n",
    "    return link.lower().endswith(\".pdf\")\n",
    "\n",
    "\n",
    "def download_pdf(url, save_folder):\n",
    "    \"\"\"Download a PDF file from the given URL.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        filename = os.path.join(save_folder, os.path.basename(urlparse(url).path))\n",
    "        with open(filename, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"Downloaded PDF: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download PDF {url}: {e}\")\n",
    "\n",
    "\n",
    "def get_page_title(soup):\n",
    "    \"\"\"Extract the title of the webpage.\"\"\"\n",
    "    title_tag = soup.find(\"title\")\n",
    "    return title_tag.get_text(strip=True) if title_tag else \"No Title\"\n",
    "\n",
    "\n",
    "def url_to_filename(url):\n",
    "    \"\"\"\n",
    "    Convert a URL to a filename-friendly string.\n",
    "    Example: 'https://example.com' -> 'example_com'\n",
    "    \"\"\"\n",
    "    parsed_url = urlparse(url)\n",
    "    domain = parsed_url.netloc  # Extract the domain\n",
    "    return domain.replace(\".\", \"_\").replace(\"www_\", \"\")\n",
    "\n",
    "\n",
    "def crawl_website(base_url, additional_domains, max_links=None):\n",
    "    \"\"\"Crawl a website and extract all text content.\"\"\"\n",
    "    # Parse the main domain from the base URL\n",
    "    parsed_base = urlparse(base_url)\n",
    "    main_domain = parsed_base.netloc\n",
    "\n",
    "    visited = set()\n",
    "    to_visit = [base_url]\n",
    "    output_text = \"\"\n",
    "\n",
    "    # Create folder for PDFs\n",
    "    pdf_folder = f\"pdfs/{url_to_filename(base_url)}\"\n",
    "    os.makedirs(pdf_folder, exist_ok=True)\n",
    "\n",
    "    crawled_count = 0  # Counter for the number of links processed\n",
    "\n",
    "    while to_visit:\n",
    "        if max_links is not None and crawled_count >= max_links:\n",
    "            print(f\"Reached the test limit of {max_links} links.\")\n",
    "            break\n",
    "\n",
    "        current_url = to_visit.pop(0)\n",
    "        if current_url in visited:\n",
    "            continue\n",
    "\n",
    "        print(f\"Visiting: {current_url}\")\n",
    "        visited.add(current_url)\n",
    "        crawled_count += 1\n",
    "\n",
    "        if is_pdf_file(current_url):\n",
    "            download_pdf(current_url, pdf_folder)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            response = requests.get(current_url)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # Extract title and content\n",
    "            title = get_page_title(soup)\n",
    "            content = soup.get_text(strip=True)\n",
    "\n",
    "            # Append to output in the specified format\n",
    "            output_text += (\n",
    "                f\"LINK: {current_url}\\nTITLE: {title}\\nCONTENT: {content}\\n\\n\"\n",
    "            )\n",
    "\n",
    "            # Find all links on the page\n",
    "            for link in soup.find_all(\"a\", href=True):\n",
    "                current_base = response.url  # Use the actual URL of the current page\n",
    "                full_url = urljoin(current_base, link[\"href\"])\n",
    "\n",
    "                # Remove fragment identifiers from the URL\n",
    "                parsed_url = urlparse(full_url)\n",
    "                full_url = urlunparse(parsed_url._replace(fragment=\"\"))\n",
    "                if (\n",
    "                    is_allowed_domain(full_url, main_domain, additional_domains)\n",
    "                    and full_url not in visited\n",
    "                ):\n",
    "                    to_visit.append(full_url)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to fetch {current_url}: {e}\")\n",
    "\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Set `max_links` for testing (e.g., 20). Set to `None` for production.\n",
    "#     max_test_links = 1000\n",
    "\n",
    "#     website_url = \"https://www.uiw.edu/hebsba/faculty-and-staff/index.html\"\n",
    "#     filename = url_to_filename(website_url)\n",
    "#     additional_domains = {\"relateddomain.com\"}\n",
    "#     result = crawl_website(website_url, additional_domains, max_links=max_test_links)\n",
    "\n",
    "#     # Save the output to a file\n",
    "#     with open(f\"crawled_content_{filename}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "#         file.write(result)\n",
    "#     print(\n",
    "#         \"Crawling complete. Content saved to 'crawled_content.txt'. PDFs are saved in 'pdfs/' folder.\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_content(soup):\n",
    "    \"\"\"\n",
    "    Format the content of a webpage by preserving headers, bullet points, and numbered lists.\n",
    "    \"\"\"\n",
    "    formatted_content = []\n",
    "\n",
    "    for element in soup.find_all(\n",
    "        [\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"p\", \"li\", \"label\"]\n",
    "    ):\n",
    "        # Add headers with line breaks\n",
    "        if element.name.startswith(\"h\"):\n",
    "            formatted_content.append(f\"\\n{element.get_text(strip=True)}\\n\")\n",
    "        # Add bullet points for list items\n",
    "        elif element.name == \"li\":\n",
    "            formatted_content.append(f\"- {element.get_text(strip=True)}\")\n",
    "        # Add paragraphs as-is\n",
    "        elif element.name == \"p\":\n",
    "            formatted_content.append(element.get_text(strip=True))\n",
    "\n",
    "    # Join all elements with newlines\n",
    "    return \"\\n\".join(formatted_content)\n",
    "\n",
    "\n",
    "def crawl_website(base_url, additional_domains, max_links=None):\n",
    "    \"\"\"Crawl a website and extract all text content.\"\"\"\n",
    "    # Parse the main domain from the base URL\n",
    "    parsed_base = urlparse(base_url)\n",
    "    main_domain = parsed_base.netloc\n",
    "\n",
    "    visited = set()\n",
    "    to_visit = [base_url]\n",
    "    output_text = \"\"\n",
    "\n",
    "    # Create folder for PDFs\n",
    "    pdf_folder = f\"pdfs/{url_to_filename(base_url)}\"\n",
    "    os.makedirs(pdf_folder, exist_ok=True)\n",
    "\n",
    "    crawled_count = 0  # Counter for the number of links processed\n",
    "\n",
    "    while to_visit:\n",
    "        if max_links is not None and crawled_count >= max_links:\n",
    "            print(f\"Reached the test limit of {max_links} links.\")\n",
    "            break\n",
    "\n",
    "        current_url = to_visit.pop(0)\n",
    "        if current_url in visited:\n",
    "            continue\n",
    "\n",
    "        print(f\"Visiting: {current_url}\")\n",
    "        visited.add(current_url)\n",
    "        crawled_count += 1\n",
    "\n",
    "        if is_pdf_file(current_url):\n",
    "            download_pdf(current_url, pdf_folder)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            response = requests.get(current_url)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # Extract title and formatted content\n",
    "            title = get_page_title(soup)\n",
    "            content = format_content(soup)\n",
    "\n",
    "            # Append to output in the specified format\n",
    "            output_text += (\n",
    "                f\"LINK: {current_url}\\nTITLE: {title}\\nCONTENT:\\n{content}\\n\\n\"\n",
    "            )\n",
    "\n",
    "            # Find all links on the page\n",
    "            for link in soup.find_all(\"a\", href=True):\n",
    "                current_base = response.url  # Use the actual URL of the current page\n",
    "                full_url = urljoin(current_base, link[\"href\"])\n",
    "\n",
    "                # Remove fragment identifiers from the URL\n",
    "                parsed_url = urlparse(full_url)\n",
    "                full_url = urlunparse(parsed_url._replace(fragment=\"\"))\n",
    "                if (\n",
    "                    is_allowed_domain(full_url, main_domain, additional_domains)\n",
    "                    and full_url not in visited\n",
    "                ):\n",
    "                    to_visit.append(full_url)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to fetch {current_url}: {e}\")\n",
    "\n",
    "    return output_text\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Set `max_links` for testing (e.g., 20). Set to `None` for production.\n",
    "#     max_test_links = 100\n",
    "\n",
    "#     website_url = \"https://www.uiw.edu/hebsba/faculty-and-staff/index.html\"\n",
    "#     filename = url_to_filename(website_url)\n",
    "#     additional_domains = {\"relateddomain.com\"}\n",
    "#     result = crawl_website(website_url, additional_domains, max_links=max_test_links)\n",
    "\n",
    "#     # Save the output to a file\n",
    "#     with open(f\"crawled_content_{filename}_4.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "#         file.write(result)\n",
    "#     print(\n",
    "#         \"Crawling complete. Content saved to 'crawled_content.txt'. PDFs are saved in 'pdfs/' folder.\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_common_sections(soup):\n",
    "    \"\"\"\n",
    "    Remove common menu, header, and footer sections from the parsed HTML.\n",
    "    \"\"\"\n",
    "    # Define common section selectors to remove\n",
    "    common_selectors = [\"header\", \"footer\", \".menu\", \".navbar\", \".footer\"]\n",
    "\n",
    "    for selector in common_selectors:\n",
    "        for element in soup.select(selector):\n",
    "            element.decompose()  # Remove the element from the DOM\n",
    "\n",
    "    return soup\n",
    "\n",
    "\n",
    "def format_content(soup):\n",
    "    \"\"\"\n",
    "    Format the content of a webpage by preserving headers, bullet points, and numbered lists.\n",
    "    \"\"\"\n",
    "    formatted_content = []\n",
    "    # TODO: the div should be based on the class and get only content if they have relevent information.\n",
    "    for element in soup.find_all(\n",
    "        [\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"p\", \"li\", \"label\", \"span\"]  # , \"div\"\n",
    "    ):\n",
    "        # Add headers with line breaks\n",
    "        if (\n",
    "            (element.name.startswith(\"h\"))\n",
    "            | (element.name == \"label\")\n",
    "            | (element.name == \"span\")\n",
    "            # | (element.name == \"div\")\n",
    "        ):\n",
    "            formatted_content.append(f\"\\n{element.get_text(strip=True)}\\n\")\n",
    "        # Add bullet points for list items\n",
    "        elif element.name == \"li\":\n",
    "            formatted_content.append(f\"- {element.get_text(strip=True)}\")\n",
    "        # Add paragraphs as-is\n",
    "        elif element.name == \"p\":\n",
    "            formatted_content.append(element.get_text(strip=True))\n",
    "        else:\n",
    "            formatted_content.append(element.get_text(strip=True))\n",
    "\n",
    "    # Join all elements with newlines\n",
    "    return \"\\n\".join(formatted_content)\n",
    "\n",
    "\n",
    "def crawl_website(base_url, additional_domains, max_links=None):\n",
    "    \"\"\"Crawl a website and extract all text content.\"\"\"\n",
    "    # Parse the main domain from the base URL\n",
    "    parsed_base = urlparse(base_url)\n",
    "    main_domain = parsed_base.netloc\n",
    "\n",
    "    visited = set()\n",
    "    to_visit = [base_url]\n",
    "    output_text = \"\"\n",
    "\n",
    "    # Create folder for PDFs\n",
    "    pdf_folder = f\"pdfs/{url_to_filename(base_url)}\"\n",
    "    os.makedirs(pdf_folder, exist_ok=True)\n",
    "\n",
    "    crawled_count = 0  # Counter for the number of links processed\n",
    "\n",
    "    while to_visit:\n",
    "        if max_links is not None and crawled_count >= max_links:\n",
    "            print(f\"Reached the test limit of {max_links} links.\")\n",
    "            break\n",
    "\n",
    "        current_url = to_visit.pop(0)\n",
    "        if current_url in visited:\n",
    "            continue\n",
    "\n",
    "        print(f\"Visiting: {current_url}\")\n",
    "        visited.add(current_url)\n",
    "        crawled_count += 1\n",
    "\n",
    "        if is_pdf_file(current_url):\n",
    "            download_pdf(current_url, pdf_folder)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            response = requests.get(current_url)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # Remove common sections\n",
    "            soup = remove_common_sections(soup)\n",
    "\n",
    "            # Extract title and formatted content\n",
    "            title = get_page_title(soup)\n",
    "            content = format_content(soup)\n",
    "\n",
    "            # Append to output in the specified format\n",
    "            output_text += (\n",
    "                f\"LINK: {current_url}\\nTITLE: {title}\\nCONTENT:\\n{content}\\n\\n\"\n",
    "            )\n",
    "\n",
    "            # Find all links on the page\n",
    "            for link in soup.find_all(\"a\", href=True):\n",
    "                current_base = response.url  # Use the actual URL of the current page\n",
    "                full_url = urljoin(current_base, link[\"href\"])\n",
    "\n",
    "                # Remove fragment identifiers from the URL\n",
    "                parsed_url = urlparse(full_url)\n",
    "                full_url = urlunparse(parsed_url._replace(fragment=\"\"))\n",
    "                if (\n",
    "                    is_allowed_domain(full_url, main_domain, additional_domains)\n",
    "                    and full_url not in visited\n",
    "                ):\n",
    "                    to_visit.append(full_url)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to fetch {current_url}: {e}\")\n",
    "\n",
    "    return output_text\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Set `max_links` for testing (e.g., 20). Set to `None` for production.\n",
    "#     max_test_links = 100\n",
    "\n",
    "#     website_url = \"https://www.uiw.edu/hebsba/faculty-and-staff/index.html\"\n",
    "#     filename = url_to_filename(website_url)\n",
    "#     additional_domains = {\"relateddomain.com\"}\n",
    "#     result = crawl_website(website_url, additional_domains, max_links=max_test_links)\n",
    "\n",
    "#     # Save the output to a file\n",
    "#     with open(f\"crawled_content_{filename}_6.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "#         file.write(result)\n",
    "#     print(\n",
    "#         \"Crawling complete. Content saved to 'crawled_content.txt'. PDFs are saved in 'pdfs/' folder.\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Set `max_links` for testing (e.g., 20). Set to `None` for production.\n",
    "#     max_test_links = 50\n",
    "\n",
    "#     website_url = \"https://www.stmary.edu/faculty/index\"\n",
    "#     filename = url_to_filename(website_url)\n",
    "#     additional_domains = {\"relateddomain.com\"}\n",
    "#     result = crawl_website(website_url, additional_domains, max_links=max_test_links)\n",
    "\n",
    "#     # Save the output to a file\n",
    "#     with open(f\"crawled_content_{filename}_6.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "#         file.write(result)\n",
    "#     print(\n",
    "#         \"Crawling complete. Content saved to 'crawled_content.txt'. PDFs are saved in 'pdfs/' folder.\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visiting: https://klesse.utsa.edu/mechanical/faculty/\n",
      "Visiting: https://klesse.utsa.edu/index.html\n",
      "Visiting: https://klesse.utsa.edu/mechanical/index.html\n",
      "Visiting: https://klesse.utsa.edu/mechanical/faculty/index.html\n",
      "Visiting: https://klesse.utsa.edu/mechanical/programs.html\n",
      "Visiting: https://klesse.utsa.edu/mechanical/information.html\n",
      "Visiting: https://klesse.utsa.edu/mechanical/faculty/advisory.html\n",
      "Visiting: https://klesse.utsa.edu/mechanical/faculty/resources.html\n",
      "Visiting: https://klesse.utsa.edu/mechanical/faculty/openings.html\n",
      "Visiting: https://klesse.utsa.edu/mechanical/research.html\n",
      "Visiting: https://klesse.utsa.edu/mechanical/students.html\n",
      "Visiting: https://klesse.utsa.edu/mechanical/contact.html\n",
      "Visiting: https://klesse.utsa.edu/requestinfo.html\n",
      "Visiting: https://klesse.utsa.edu/faculty/index.html\n",
      "Visiting: https://klesse.utsa.edu/faculty/profiles/abbas-omar.html\n",
      "Visiting: https://klesse.utsa.edu/faculty/profiles/araya-guillermo.html\n",
      "Visiting: https://klesse.utsa.edu/faculty/profiles/axler-keith.html\n",
      "Visiting: https://klesse.utsa.edu/faculty/profiles/bhaganagar-kiran.html\n",
      "Visiting: https://klesse.utsa.edu/faculty/profiles/bhuiyan-tanveer-hossain.html\n",
      "Visiting: https://klesse.utsa.edu/faculty/profiles/castillo-krystel.html\n",
      "Visiting: https://klesse.utsa.edu/faculty/profiles/chen-f-frank.html\n",
      "Visiting: https://klesse.utsa.edu/faculty/profiles/chocron-sidney.html\n",
      "Visiting: https://klesse.utsa.edu/faculty/profiles/combs-christopher.html\n",
      "Visiting: https://klesse.utsa.edu/faculty/profiles/crom-alifer.html\n",
      "Visiting: https://klesse.utsa.edu/faculty/profiles/de-lorenzo-robert.html\n",
      "Visiting: https://klesse.utsa.edu/faculty/profiles/feng-zhi-gang.html\n",
      "Visiting: https://klesse.utsa.edu/faculty/profiles/finol-ender.html\n",
      "Visiting: https://klesse.utsa.edu/faculty/profiles/gonzalez-cody.html\n",
      "Visiting: https://klesse.utsa.edu/faculty/profiles/govindaraju-madhavrao.html\n",
      "Visiting: https://klesse.utsa.edu/faculty/profiles/han-hai-chao.html\n",
      "Visiting: https://klesse.utsa.edu/faculty/profiles/herbert-francisco.html\n",
      "Visiting: https://klesse.utsa.edu/faculty/profiles/hood-r-lyle.html\n",
      "Visiting: https://klesse.utsa.edu/faculty/profiles/karimi-amir.html\n",
      "Visiting: https://klesse.utsa.edu/faculty/profiles/manteufel-randall.html\n",
      "Visiting: https://klesse.utsa.edu/faculty/profiles/millwater-harry.html\n",
      "Visiting: https://klesse.utsa.edu/faculty/profiles/seidi-morteza.html\n",
      "Visiting: https://klesse.utsa.edu/faculty/profiles/nedungadi-ashok.html\n",
      "Visiting: https://klesse.utsa.edu/faculty/profiles/pineda-daniel.html\n",
      "Visiting: https://klesse.utsa.edu/faculty/profiles/restrepo-david.html\n",
      "Visiting: https://klesse.utsa.edu/faculty/profiles/singh-hardev.html\n",
      "Reached the test limit of 40 links.\n",
      "Crawling complete. Content saved to 'crawled_content.txt'. PDFs are saved in 'pdfs/' folder.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Set `max_links` for testing (e.g., 20). Set to `None` for production.\n",
    "    max_test_links = 40\n",
    "\n",
    "    website_url = \"https://klesse.utsa.edu/mechanical/faculty/\"\n",
    "    filename = url_to_filename(website_url)\n",
    "    additional_domains = {\"relateddomain.com\"}\n",
    "    result = crawl_website(website_url, additional_domains, max_links=max_test_links)\n",
    "\n",
    "    # Save the output to a file\n",
    "    with open(f\"crawled_content_{filename}_1.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(result)\n",
    "    print(\n",
    "        \"Crawling complete. Content saved to 'crawled_content.txt'. PDFs are saved in 'pdfs/' folder.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mma_ufc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
