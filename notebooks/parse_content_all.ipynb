{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse, urlunparse\n",
    "\n",
    "\n",
    "def is_allowed_domain(link, main_domain, additional_domains):\n",
    "    \"\"\"\n",
    "    Check if the link belongs to the main domain or its subdomains,\n",
    "    or any of the additional allowed domains.\n",
    "    \"\"\"\n",
    "    parsed_link = urlparse(link)\n",
    "    domain = parsed_link.netloc\n",
    "\n",
    "    # Allow the main domain and all its subdomains\n",
    "    if domain == main_domain or domain.endswith(f\".{main_domain}\"):\n",
    "        return True\n",
    "\n",
    "    # Allow explicitly allowed additional domains\n",
    "    return any(\n",
    "        domain == add_domain or domain.endswith(f\".{add_domain}\")\n",
    "        for add_domain in additional_domains\n",
    "    )\n",
    "\n",
    "\n",
    "def is_pdf_file(link):\n",
    "    \"\"\"Check if the link points to a PDF file.\"\"\"\n",
    "    return link.lower().endswith(\".pdf\")\n",
    "\n",
    "\n",
    "def download_pdf(url, save_folder):\n",
    "    \"\"\"Download a PDF file from the given URL.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        filename = os.path.join(save_folder, os.path.basename(urlparse(url).path))\n",
    "        with open(filename, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"Downloaded PDF: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download PDF {url}: {e}\")\n",
    "\n",
    "\n",
    "def get_page_title(soup):\n",
    "    \"\"\"Extract the title of the webpage.\"\"\"\n",
    "    title_tag = soup.find(\"title\")\n",
    "    return title_tag.get_text(strip=True) if title_tag else \"No Title\"\n",
    "\n",
    "\n",
    "def url_to_filename(url):\n",
    "    \"\"\"\n",
    "    Convert a URL to a filename-friendly string.\n",
    "    Example: 'https://example.com' -> 'example_com'\n",
    "    \"\"\"\n",
    "    parsed_url = urlparse(url)\n",
    "    domain = parsed_url.netloc  # Extract the domain\n",
    "    return domain.replace(\".\", \"_\").replace(\"www_\", \"\")\n",
    "\n",
    "\n",
    "def crawl_website(base_url, additional_domains, max_links=None):\n",
    "    \"\"\"Crawl a website and extract all text content.\"\"\"\n",
    "    # Parse the main domain from the base URL\n",
    "    parsed_base = urlparse(base_url)\n",
    "    main_domain = parsed_base.netloc\n",
    "\n",
    "    visited = set()\n",
    "    to_visit = [base_url]\n",
    "    output_text = \"\"\n",
    "\n",
    "    # Create folder for PDFs\n",
    "    pdf_folder = f\"pdfs/{url_to_filename(base_url)}\"\n",
    "    os.makedirs(pdf_folder, exist_ok=True)\n",
    "\n",
    "    crawled_count = 0  # Counter for the number of links processed\n",
    "\n",
    "    while to_visit:\n",
    "        if max_links is not None and crawled_count >= max_links:\n",
    "            print(f\"Reached the test limit of {max_links} links.\")\n",
    "            break\n",
    "\n",
    "        current_url = to_visit.pop(0)\n",
    "        if current_url in visited:\n",
    "            continue\n",
    "\n",
    "        print(f\"Visiting: {current_url}\")\n",
    "        visited.add(current_url)\n",
    "        crawled_count += 1\n",
    "\n",
    "        if is_pdf_file(current_url):\n",
    "            download_pdf(current_url, pdf_folder)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            response = requests.get(current_url)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # Extract title and content\n",
    "            title = get_page_title(soup)\n",
    "            content = soup.get_text(strip=True)\n",
    "\n",
    "            # Append to output in the specified format\n",
    "            output_text += (\n",
    "                f\"LINK: {current_url}\\nTITLE: {title}\\nCONTENT: {content}\\n\\n\"\n",
    "            )\n",
    "\n",
    "            # Find all links on the page\n",
    "            for link in soup.find_all(\"a\", href=True):\n",
    "                current_base = response.url  # Use the actual URL of the current page\n",
    "                full_url = urljoin(current_base, link[\"href\"])\n",
    "\n",
    "                # Remove fragment identifiers from the URL\n",
    "                parsed_url = urlparse(full_url)\n",
    "                full_url = urlunparse(parsed_url._replace(fragment=\"\"))\n",
    "                if (\n",
    "                    is_allowed_domain(full_url, main_domain, additional_domains)\n",
    "                    and full_url not in visited\n",
    "                ):\n",
    "                    to_visit.append(full_url)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to fetch {current_url}: {e}\")\n",
    "\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Set `max_links` for testing (e.g., 20). Set to `None` for production.\n",
    "#     max_test_links = 1000\n",
    "\n",
    "#     website_url = \"https://www.uiw.edu/hebsba/faculty-and-staff/index.html\"\n",
    "#     filename = url_to_filename(website_url)\n",
    "#     additional_domains = {\"relateddomain.com\"}\n",
    "#     result = crawl_website(website_url, additional_domains, max_links=max_test_links)\n",
    "\n",
    "#     # Save the output to a file\n",
    "#     with open(f\"crawled_content_{filename}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "#         file.write(result)\n",
    "#     print(\n",
    "#         \"Crawling complete. Content saved to 'crawled_content.txt'. PDFs are saved in 'pdfs/' folder.\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def format_content(soup):\n",
    "#     \"\"\"\n",
    "#     Format the content of a webpage by preserving headers, bullet points, and numbered lists.\n",
    "#     \"\"\"\n",
    "#     formatted_content = []\n",
    "\n",
    "#     for element in soup.find_all(\n",
    "#         [\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"p\", \"li\", \"label\"]\n",
    "#     ):\n",
    "#         # Add headers with line breaks\n",
    "#         if element.name.startswith(\"h\"):\n",
    "#             formatted_content.append(f\"\\n{element.get_text(strip=True)}\\n\")\n",
    "#         # Add bullet points for list items\n",
    "#         elif element.name == \"li\":\n",
    "#             formatted_content.append(f\"- {element.get_text(strip=True)}\")\n",
    "#         # Add paragraphs as-is\n",
    "#         elif element.name == \"p\":\n",
    "#             formatted_content.append(element.get_text(strip=True))\n",
    "\n",
    "#     # Join all elements with newlines\n",
    "#     return \"\\n\".join(formatted_content)\n",
    "\n",
    "\n",
    "# def crawl_website(base_url, additional_domains, max_links=None):\n",
    "#     \"\"\"Crawl a website and extract all text content.\"\"\"\n",
    "#     # Parse the main domain from the base URL\n",
    "#     parsed_base = urlparse(base_url)\n",
    "#     main_domain = parsed_base.netloc\n",
    "\n",
    "#     visited = set()\n",
    "#     to_visit = [base_url]\n",
    "#     output_text = \"\"\n",
    "\n",
    "#     # Create folder for PDFs\n",
    "#     pdf_folder = f\"pdfs/{url_to_filename(base_url)}\"\n",
    "#     os.makedirs(pdf_folder, exist_ok=True)\n",
    "\n",
    "#     crawled_count = 0  # Counter for the number of links processed\n",
    "\n",
    "#     while to_visit:\n",
    "#         if max_links is not None and crawled_count >= max_links:\n",
    "#             print(f\"Reached the test limit of {max_links} links.\")\n",
    "#             break\n",
    "\n",
    "#         current_url = to_visit.pop(0)\n",
    "#         if current_url in visited:\n",
    "#             continue\n",
    "\n",
    "#         print(f\"Visiting: {current_url}\")\n",
    "#         visited.add(current_url)\n",
    "#         crawled_count += 1\n",
    "\n",
    "#         if is_pdf_file(current_url):\n",
    "#             download_pdf(current_url, pdf_folder)\n",
    "#             continue\n",
    "\n",
    "#         try:\n",
    "#             response = requests.get(current_url)\n",
    "#             response.raise_for_status()\n",
    "#             soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "#             # Extract title and formatted content\n",
    "#             title = get_page_title(soup)\n",
    "#             content = format_content(soup)\n",
    "\n",
    "#             # Append to output in the specified format\n",
    "#             output_text += (\n",
    "#                 f\"LINK: {current_url}\\nTITLE: {title}\\nCONTENT:\\n{content}\\n\\n\"\n",
    "#             )\n",
    "\n",
    "#             # Find all links on the page\n",
    "#             for link in soup.find_all(\"a\", href=True):\n",
    "#                 current_base = response.url  # Use the actual URL of the current page\n",
    "#                 full_url = urljoin(current_base, link[\"href\"])\n",
    "\n",
    "#                 # Remove fragment identifiers from the URL\n",
    "#                 parsed_url = urlparse(full_url)\n",
    "#                 full_url = urlunparse(parsed_url._replace(fragment=\"\"))\n",
    "#                 if (\n",
    "#                     is_allowed_domain(full_url, main_domain, additional_domains)\n",
    "#                     and full_url not in visited\n",
    "#                 ):\n",
    "#                     to_visit.append(full_url)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Failed to fetch {current_url}: {e}\")\n",
    "\n",
    "#     return output_text\n",
    "\n",
    "\n",
    "# # # Example usage\n",
    "# # if __name__ == \"__main__\":\n",
    "# #     # Set `max_links` for testing (e.g., 20). Set to `None` for production.\n",
    "# #     max_test_links = 100\n",
    "\n",
    "# #     website_url = \"https://www.uiw.edu/hebsba/faculty-and-staff/index.html\"\n",
    "# #     filename = url_to_filename(website_url)\n",
    "# #     additional_domains = {\"relateddomain.com\"}\n",
    "# #     result = crawl_website(website_url, additional_domains, max_links=max_test_links)\n",
    "\n",
    "# #     # Save the output to a file\n",
    "# #     with open(f\"crawled_content_{filename}_4.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "# #         file.write(result)\n",
    "# #     print(\n",
    "# #         \"Crawling complete. Content saved to 'crawled_content.txt'. PDFs are saved in 'pdfs/' folder.\"\n",
    "# #     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_common_sections(soup):\n",
    "    \"\"\"\n",
    "    Remove common menu, header, and footer sections from the parsed HTML.\n",
    "    \"\"\"\n",
    "    # Define common section selectors to remove\n",
    "    common_selectors = [\"header\", \"footer\", \".menu\", \".navbar\", \".footer\"]\n",
    "\n",
    "    for selector in common_selectors:\n",
    "        for element in soup.select(selector):\n",
    "            element.decompose()  # Remove the element from the DOM\n",
    "\n",
    "    return soup\n",
    "\n",
    "\n",
    "def format_content(soup):\n",
    "    \"\"\"\n",
    "    Format the content of a webpage by preserving headers, bullet points, and numbered lists.\n",
    "    \"\"\"\n",
    "    formatted_content = []\n",
    "    # TODO: the div should be based on the class and get only content if they have relevent information.\n",
    "    for element in soup.find_all(\n",
    "        [\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"p\", \"li\", \"label\", \"span\", \"div\"]  # , \n",
    "    ):\n",
    "        # Add headers with line breaks\n",
    "        if (\n",
    "            (element.name.startswith(\"h\"))\n",
    "            | (element.name == \"label\")\n",
    "            | (element.name == \"span\")\n",
    "            | (element.name == \"div\")\n",
    "        ):\n",
    "            formatted_content.append(f\"\\n{element.get_text(strip=True)}\\n\")\n",
    "        # Add bullet points for list items\n",
    "        elif element.name == \"li\":\n",
    "            formatted_content.append(f\"- {element.get_text(strip=True)}\")\n",
    "        # Add paragraphs as-is\n",
    "        elif element.name == \"p\":\n",
    "            formatted_content.append(element.get_text(strip=True))\n",
    "        else:\n",
    "            formatted_content.append(element.get_text(strip=True))\n",
    "\n",
    "    # Join all elements with newlines\n",
    "    return \"\\n\".join(formatted_content)\n",
    "\n",
    "\n",
    "def crawl_website(base_url, additional_domains, max_links=None):\n",
    "    \"\"\"Crawl a website and extract all text content.\"\"\"\n",
    "    # Parse the main domain from the base URL\n",
    "    parsed_base = urlparse(base_url)\n",
    "    main_domain = parsed_base.netloc\n",
    "\n",
    "    visited = set()\n",
    "    to_visit = [base_url]\n",
    "    output_text = \"\"\n",
    "\n",
    "    # Create folder for PDFs\n",
    "    pdf_folder = f\"pdfs/{url_to_filename(base_url)}\"\n",
    "    os.makedirs(pdf_folder, exist_ok=True)\n",
    "\n",
    "    crawled_count = 0  # Counter for the number of links processed\n",
    "\n",
    "    while to_visit:\n",
    "        if max_links is not None and crawled_count >= max_links:\n",
    "            print(f\"Reached the test limit of {max_links} links.\")\n",
    "            break\n",
    "\n",
    "        current_url = to_visit.pop(0)\n",
    "        if current_url in visited:\n",
    "            continue\n",
    "\n",
    "        print(f\"Visiting: {current_url}\")\n",
    "        visited.add(current_url)\n",
    "        crawled_count += 1\n",
    "\n",
    "        if is_pdf_file(current_url):\n",
    "            download_pdf(current_url, pdf_folder)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            response = requests.get(current_url)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # Remove common sections\n",
    "            soup = remove_common_sections(soup)\n",
    "\n",
    "            # Extract title and formatted content\n",
    "            title = get_page_title(soup)\n",
    "            content = format_content(soup)\n",
    "\n",
    "            # Append to output in the specified format\n",
    "            output_text += (\n",
    "                f\"LINK: {current_url}\\nTITLE: {title}\\nCONTENT:\\n{content}\\n\\n\"\n",
    "            )\n",
    "\n",
    "            # Find all links on the page\n",
    "            for link in soup.find_all(\"a\", href=True):\n",
    "                current_base = response.url  # Use the actual URL of the current page\n",
    "                full_url = urljoin(current_base, link[\"href\"])\n",
    "\n",
    "                # Remove fragment identifiers from the URL\n",
    "                parsed_url = urlparse(full_url)\n",
    "                full_url = urlunparse(parsed_url._replace(fragment=\"\"))\n",
    "                if (\n",
    "                    is_allowed_domain(full_url, main_domain, additional_domains)\n",
    "                    and full_url not in visited\n",
    "                ):\n",
    "                    to_visit.append(full_url)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to fetch {current_url}: {e}\")\n",
    "\n",
    "    return output_text\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Set `max_links` for testing (e.g., 20). Set to `None` for production.\n",
    "#     max_test_links = 100\n",
    "\n",
    "#     website_url = \"https://www.uiw.edu/hebsba/faculty-and-staff/index.html\"\n",
    "#     filename = url_to_filename(website_url)\n",
    "#     additional_domains = {\"relateddomain.com\"}\n",
    "#     result = crawl_website(website_url, additional_domains, max_links=max_test_links)\n",
    "\n",
    "#     # Save the output to a file\n",
    "#     with open(f\"crawled_content_{filename}_6.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "#         file.write(result)\n",
    "#     print(\n",
    "#         \"Crawling complete. Content saved to 'crawled_content.txt'. PDFs are saved in 'pdfs/' folder.\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Set `max_links` for testing (e.g., 20). Set to `None` for production.\n",
    "#     max_test_links = 50\n",
    "\n",
    "#     website_url = \"https://www.stmary.edu/faculty/index\"\n",
    "#     filename = url_to_filename(website_url)\n",
    "#     additional_domains = {\"relateddomain.com\"}\n",
    "#     result = crawl_website(website_url, additional_domains, max_links=max_test_links)\n",
    "\n",
    "#     # Save the output to a file\n",
    "#     with open(f\"crawled_content_{filename}_6.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "#         file.write(result)\n",
    "#     print(\n",
    "#         \"Crawling complete. Content saved to 'crawled_content.txt'. PDFs are saved in 'pdfs/' folder.\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Set `max_links` for testing (e.g., 20). Set to `None` for production.\n",
    "#     max_test_links = 40\n",
    "\n",
    "#     website_url = \"https://klesse.utsa.edu/mechanical/faculty/\"\n",
    "#     filename = url_to_filename(website_url)\n",
    "#     additional_domains = {\"relateddomain.com\"}\n",
    "#     result = crawl_website(website_url, additional_domains, max_links=max_test_links)\n",
    "\n",
    "#     # Save the output to a file\n",
    "#     with open(f\"../data/crawled_content/{filename}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "#         file.write(result)\n",
    "#     print(\n",
    "#         \"Crawling complete. Content saved to 'crawled_content.txt'. PDFs are saved in 'pdfs/' folder.\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_common_sections(soup):\n",
    "    \"\"\"\n",
    "    Remove common menu, header, and footer sections from the parsed HTML.\n",
    "    \"\"\"\n",
    "    # Define common section selectors to remove\n",
    "    common_selectors = [\"header\", \"footer\", \".menu\", \".navbar\", \".footer\"]\n",
    "\n",
    "    for selector in common_selectors:\n",
    "        for element in soup.select(selector):\n",
    "            element.decompose()  # Remove the element from the DOM\n",
    "\n",
    "    return soup\n",
    "\n",
    "\n",
    "def is_relevant_content(text):\n",
    "    \"\"\"\n",
    "    Check if the text is relevant and not boilerplate or empty.\n",
    "    \"\"\"\n",
    "    irrelevant_phrases = [\n",
    "        \"top of the page\",\n",
    "        \"skip to content\",\n",
    "        \"loading...\",\n",
    "        \"menu\",\n",
    "        \"navigation\",\n",
    "    ]\n",
    "    # Check if the text is non-empty and not in the irrelevant phrases\n",
    "    return text and text.strip() and not any(phrase in text.lower() for phrase in irrelevant_phrases)\n",
    "\n",
    "def format_content(soup):\n",
    "    \"\"\"\n",
    "    Format the content of a webpage by preserving headers, bullet points, and numbered lists,\n",
    "    while avoiding duplicate and irrelevant content, and normalizing spaces.\n",
    "    \"\"\"\n",
    "    formatted_content = []\n",
    "    seen_content = set()  # Track already added content to avoid duplicates\n",
    "\n",
    "    for element in soup.find_all([\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"p\", \"li\", \"label\", \"span\", \"div\"]):\n",
    "        # Skip tags that have child elements already processed\n",
    "        if element.findChildren():\n",
    "            continue\n",
    "\n",
    "        # Extract and normalize text\n",
    "        text = \" \".join(element.stripped_strings)  # Combine child strings with spaces\n",
    "        text = re.sub(r\"\\s+\", \" \", text)  # Replace multiple spaces/newlines with a single space\n",
    "        text = text.strip()\n",
    "\n",
    "        # Skip if the text is irrelevant or already seen\n",
    "        if is_relevant_content(text) and text not in seen_content:\n",
    "            # Add headers with line breaks\n",
    "            if element.name.startswith(\"h\"):\n",
    "                formatted_content.append(f\"\\n{text}\\n\")\n",
    "            # Add bullet points for list items\n",
    "            elif element.name == \"li\":\n",
    "                formatted_content.append(f\"- {text}\")\n",
    "            # Add paragraphs or other content as-is\n",
    "            else:\n",
    "                formatted_content.append(text)\n",
    "            seen_content.add(text)  # Mark content as added\n",
    "\n",
    "    # Join all elements with newlines\n",
    "    return \"\\n\".join(formatted_content)\n",
    "\n",
    "\n",
    "\n",
    "def crawl_website(base_url, additional_domains, max_links=None):\n",
    "    \"\"\"Crawl a website and extract all text content.\"\"\"\n",
    "    # Parse the main domain from the base URL\n",
    "    parsed_base = urlparse(base_url)\n",
    "    main_domain = parsed_base.netloc\n",
    "\n",
    "    visited = set()\n",
    "    to_visit = [base_url]\n",
    "    output_text = \"\"\n",
    "\n",
    "    # Create folder for PDFs\n",
    "    pdf_folder = f\"pdfs/{url_to_filename(base_url)}\"\n",
    "    os.makedirs(pdf_folder, exist_ok=True)\n",
    "\n",
    "    crawled_count = 0  # Counter for the number of links processed\n",
    "\n",
    "    while to_visit:\n",
    "        if max_links is not None and crawled_count >= max_links:\n",
    "            print(f\"Reached the test limit of {max_links} links.\")\n",
    "            break\n",
    "\n",
    "        current_url = to_visit.pop(0)\n",
    "        if current_url in visited:\n",
    "            continue\n",
    "\n",
    "        print(f\"Visiting: {current_url}\")\n",
    "        visited.add(current_url)\n",
    "        crawled_count += 1\n",
    "\n",
    "        if is_pdf_file(current_url):\n",
    "            download_pdf(current_url, pdf_folder)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            response = requests.get(current_url)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # Remove common sections\n",
    "            soup = remove_common_sections(soup)\n",
    "\n",
    "            # Extract title and formatted content\n",
    "            title = get_page_title(soup)\n",
    "            content = format_content(soup)\n",
    "\n",
    "            # Append to output in the specified format\n",
    "            output_text += (\n",
    "                f\"LINK: {current_url}\\nTITLE: {title}\\nCONTENT:\\n{content}\\n\\n\"\n",
    "            )\n",
    "\n",
    "            # Find all links on the page\n",
    "            for link in soup.find_all(\"a\", href=True):\n",
    "                current_base = response.url  # Use the actual URL of the current page\n",
    "                full_url = urljoin(current_base, link[\"href\"])\n",
    "\n",
    "                # Remove fragment identifiers from the URL\n",
    "                parsed_url = urlparse(full_url)\n",
    "                full_url = urlunparse(parsed_url._replace(fragment=\"\"))\n",
    "                if (\n",
    "                    is_allowed_domain(full_url, main_domain, additional_domains)\n",
    "                    and full_url not in visited\n",
    "                ):\n",
    "                    to_visit.append(full_url)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to fetch {current_url}: {e}\")\n",
    "\n",
    "    return output_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visiting: https://www.uiw.edu/hebsba/faculty-and-staff/index.html\n",
      "Visiting: https://www.uiw.edu/\n",
      "Visiting: https://www.uiw.edu/hebsba/faculty-and-staff/departments/index.html\n",
      "Visiting: https://www.uiw.edu/hebsba/faculty-and-staff/directory/index.html\n",
      "Visiting: https://www.uiw.edu/hebsba/faculty-and-staff/professors-emeriti.html\n",
      "Visiting: https://www.uiw.edu/hebsba/index.html\n",
      "Visiting: https://www.uiw.edu/hebsba/faculty-and-staff/directory/scott-jeannie.html\n",
      "Visiting: https://www.uiw.edu/hebsba/faculty-and-staff/directory/collins-taylor.html\n",
      "Visiting: https://www.uiw.edu/hebsba/faculty-and-staff/directory/ayres-haley.html\n",
      "Visiting: https://www.uiw.edu/hebsba/faculty-and-staff/directory/bazzy-joshua.html\n",
      "Visiting: https://www.uiw.edu/hebsba/faculty-and-staff/directory/carlos-baldo-ph.d.html\n",
      "Visiting: https://www.uiw.edu/hebsba/faculty-and-staff/directory/downs-lynn.html\n",
      "Visiting: https://www.uiw.edu/hebsba/faculty-and-staff/directory/driskill-trish.html\n",
      "Visiting: https://www.uiw.edu/hebsba/faculty-and-staff/directory/edmond-tracie.html\n",
      "Visiting: https://www.uiw.edu/hebsba/faculty-and-staff/directory/forrest-michael.html\n",
      "Visiting: https://www.uiw.edu/hebsba/faculty-and-staff/directory/ghiasi-akbar.html\n",
      "Visiting: https://www.uiw.edu/hebsba/faculty-and-staff/directory/griesdorn-tim.html\n",
      "Visiting: https://www.uiw.edu/hebsba/faculty-and-staff/directory/griffiths-randall.html\n",
      "Visiting: https://www.uiw.edu/hebsba/faculty-and-staff/directory/harmsen-earl.html\n",
      "Visiting: https://www.uiw.edu/hebsba/faculty-and-staff/directory/harrison-teresa.html\n",
      "Visiting: https://www.uiw.edu/hebsba/faculty-and-staff/directory/jones-alan.html\n",
      "Visiting: https://www.uiw.edu/hebsba/faculty-and-staff/directory/kiser-angelina.html\n",
      "Visiting: https://www.uiw.edu/hebsba/faculty-and-staff/directory/liu-yi.html\n",
      "Visiting: https://www.uiw.edu/hebsba/faculty-and-staff/directory/moreno-jose.html\n",
      "Visiting: https://www.uiw.edu/hebsba/faculty-and-staff/directory/nesser-chris.html\n",
      "Visiting: https://www.uiw.edu/hebsba/faculty-and-staff/directory/norris-jt.html\n",
      "Visiting: https://www.uiw.edu/hebsba/faculty-and-staff/directory/ortega-ivan.html\n",
      "Visiting: https://www.uiw.edu/hebsba/faculty-and-staff/directory/pittman-kelly.html\n",
      "Visiting: https://www.uiw.edu/hebsba/faculty-and-staff/directory/poe-april.html\n",
      "Visiting: https://www.uiw.edu/hebsba/faculty-and-staff/directory/robin-guerrero-ph.d.html\n",
      "Visiting: https://www.uiw.edu/hebsba/faculty-and-staff/directory/rubio-alberto.html\n",
      "Visiting: https://www.uiw.edu/hebsba/faculty-and-staff/directory/rubio-alicia.html\n",
      "Visiting: https://www.uiw.edu/hebsba/faculty-and-staff/directory/scott-roberts,-ph.d..html\n",
      "Visiting: https://www.uiw.edu/hebsba/faculty-and-staff/directory/vequist-david.html\n",
      "Visiting: https://www.uiw.edu/hebsba/faculty-and-staff/directory/zanca-nursen-albayrak.html\n",
      "Visiting: https://www.uiw.edu/gouiw/apply.html\n",
      "Visiting: https://www.uiw.edu/admissions/eventregistration.html\n",
      "Visiting: https://www.uiw.edu/admissions/visit.html\n",
      "Visiting: https://www.uiw.edu/numbers/index.html\n",
      "Visiting: https://www.uiw.edu/news/2024/11/uiw-military-ranked-no-1-by-national.html\n",
      "Visiting: https://www.uiw.edu/finaid/_archive/estimateyourcosts2.html\n",
      "Visiting: https://www.uiw.edu/admissions/index2.html\n",
      "Visiting: https://www.uiw.edu/admissions/freshman-admissions/index.html\n",
      "Visiting: https://www.uiw.edu/admissions/graduate/index.html\n",
      "Visiting: https://www.uiw.edu/admissions/internationaladmissions/index.html\n",
      "Visiting: https://www.uiw.edu/admissions/_archive/eventregistration.html\n",
      "Visiting: https://www.uiw.edu/chass/index.html\n",
      "Visiting: https://www.uiw.edu/education/index.html\n",
      "Visiting: https://www.uiw.edu/smse/index.html\n",
      "Visiting: https://www.uiw.edu/smd/index.html\n",
      "Visiting: https://www.uiw.edu/admissions/resources/uiw-admissions-in-your-area.html\n",
      "Visiting: https://www.uiw.edu/one-stop/index.html\n",
      "Visiting: https://www.uiw.edu/healthprofessions/index.html\n",
      "Visiting: https://www.uiw.edu/news/2024/12/expressing-gratitude-ettling-center-distributes-holiday-meals-to-community-members.html\n",
      "Visiting: https://www.uiw.edu/news/2024/12/uiw-set-to-honor-fall-class-of-2024.html\n",
      "Visiting: https://www.uiw.edu/news/2024/12/making-dreams-a-reality-uiw-to-welcome-24-global-online-graduates-at-fall-2024-commencement-ceremony.html\n",
      "Visiting: https://www.uiw.edu/news/index.html\n",
      "Visiting: https://www.uiw.edu/about/index.html\n",
      "Visiting: https://www.uiw.edu/preparedness/index.html\n",
      "Visiting: https://www.uiw.edu/hebsba/faculty-and-staff/departments/deans-office.html\n",
      "Visiting: https://www.uiw.edu/hebsba/faculty-and-staff/departments/accounting.html\n",
      "Visiting: https://www.uiw.edu/hebsba/faculty-and-staff/departments/business-law.html\n",
      "Visiting: https://www.uiw.edu/hebsba/faculty-and-staff/departments/economics.html\n",
      "Visiting: https://www.uiw.edu/hebsba/faculty-and-staff/departments/finance.html\n",
      "Visiting: https://www.uiw.edu/hebsba/faculty-and-staff/departments/health-administration.html\n",
      "Visiting: https://www.uiw.edu/hebsba/faculty-and-staff/departments/international-business.html\n",
      "Visiting: https://www.uiw.edu/hebsba/faculty-and-staff/departments/management.html\n",
      "Visiting: https://www.uiw.edu/hebsba/faculty-and-staff/departments/marketing.html\n",
      "Visiting: https://www.uiw.edu/hebsba/faculty-and-staff/departments/management-information-systems.html\n",
      "Visiting: https://www.uiw.edu/hebsba/faculty-and-staff/departments/sport-management.html\n",
      "Visiting: https://www.uiw.edu/hebsba/news-and-events/news/2018/dan-dominguez-to-retire-hebsba-uiw-mha.html\n",
      "Visiting: https://www.uiw.edu/hebsba/news-and-events/news/2018/dr-michael-mcguire-retires-after-30-years-at-uiw-hebsba.html\n",
      "Visiting: https://www.uiw.edu/hebsba/academics/undergraduate/bachelor-of-business/index.html\n",
      "Visiting: https://www.uiw.edu/hebsba/academics/graduate/index.html\n",
      "Visiting: https://www.uiw.edu/hebsba/academics/5-year-bachelors-to-masters-program.html\n",
      "Visiting: https://www.uiw.edu/hebsba/students/index.html\n",
      "Visiting: https://www.uiw.edu/hebsba/academics/undergraduate/index.html\n",
      "Visiting: https://www.uiw.edu/hebsba/about/accreditation.html\n",
      "Visiting: https://www.uiw.edu/news/2024/11/uiw-hosts-2024-business-startup-challenge.html\n",
      "Visiting: https://www.uiw.edu/news/2024/10/honoring-adam-guerreros-legacy-graduate-accounting-students-receive-25,000-in-scholarship-support-in-honor-of-uiw-alumnus.html\n",
      "Visiting: https://www.uiw.edu/news/2024/03/uiw-mha-student-team-takes-first-place-in-the-trinity-university-healthcare-prism-pitch-competition.html\n",
      "Visiting: https://www.uiw.edu/hebsba/news-and-events/news/2022/09/assistant-professor-in-hebsba-publishes-manuscript.html\n",
      "Visiting: https://www.uiw.edu/hebsba/news-and-events/news/index.html\n",
      "Visiting: https://www.uiw.edu/hebsba/about/deans-message.html\n",
      "Visiting: https://www.uiw.edu/index.html\n",
      "Visiting: https://www.uiw.edu/admissions/resources/check-application-status.html\n",
      "Visiting: https://www.uiw.edu/admissions/graduate/nextsteps.html\n",
      "Visiting: https://www.uiw.edu/admissions/graduate/gradrequirement.html\n",
      "Visiting: https://www.uiw.edu/military-veteran-services/admissions.html\n",
      "Visiting: https://www.uiw.edu/admissions/index.html\n",
      "Visiting: https://www.uiw.edu/admissions/transfer-admissions/index.html\n",
      "Visiting: https://www.uiw.edu/admissions/readmission/index.html\n",
      "Visiting: https://www.uiw.edu/admissions/brainpower-connection/index.html\n",
      "Visiting: https://www.uiw.edu/admissions/early-college/index.html\n",
      "Visiting: https://www.uiw.edu/admissions/new-student-orientation.html\n",
      "Visiting: https://www.uiw.edu/admissions/uiw-parent-family-association/index.html\n",
      "Visiting: https://www.uiw.edu/admissions/resources/index.html\n",
      "Visiting: https://www.uiw.edu/admissions/admissions-en-espanol.html\n",
      "Visiting: https://www.uiw.edu/admissions/check-application-status.html\n",
      "Visiting: https://www.uiw.edu/admissions/resources/uiw-welcome-center.html\n",
      "Reached the test limit of 100 links.\n",
      "Crawling complete. Content saved to 'crawled_content.txt'. PDFs are saved in 'pdfs/' folder.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Set `max_links` for testing (e.g., 20). Set to `None` for production.\n",
    "    max_test_links = 100\n",
    "\n",
    "    website_url = \"https://www.uiw.edu/hebsba/faculty-and-staff/index.html\" #\"https://www.stmary.edu/faculty/index\" #\"https://klesse.utsa.edu/mechanical/faculty/\"\n",
    "    filename = url_to_filename(website_url)\n",
    "    additional_domains = {\"relateddomain.com\"}\n",
    "    result = crawl_website(website_url, additional_domains, max_links=max_test_links)\n",
    "\n",
    "    # Save the output to a file\n",
    "    with open(f\"../data/crawled_content/{filename}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(result)\n",
    "    print(\n",
    "        \"Crawling complete. Content saved to 'crawled_content.txt'. PDFs are saved in 'pdfs/' folder.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
