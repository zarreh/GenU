{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse, urlunparse\n",
    "import re\n",
    "\n",
    "\n",
    "import tldextract\n",
    "\n",
    "\n",
    "def get_main_domain(url):\n",
    "    \"\"\"\n",
    "    Extract the main domain name from a URL, removing subdomains and 'www'.\n",
    "    Example:\n",
    "        - 'https://www.tamu.edu/' -> 'tamu.edu'\n",
    "        - 'https://grad.tamu.edu/' -> 'tamu.edu'\n",
    "    \"\"\"\n",
    "    # Use tldextract to parse the URL\n",
    "    extracted = tldextract.extract(url)\n",
    "    # Combine domain and suffix (e.g., 'tamu' + 'edu')\n",
    "    return f\"{extracted.domain}.{extracted.suffix}\"\n",
    "\n",
    "\n",
    "def is_allowed_domain(link, main_domain, additional_domains):\n",
    "    \"\"\"\n",
    "    Check if the link belongs to the main domain or its subdomains,\n",
    "    or any of the additional allowed domains.\n",
    "    \"\"\"\n",
    "    parsed_link = urlparse(link)\n",
    "    domain = parsed_link.netloc\n",
    "\n",
    "    # Allow the main domain and all its subdomains\n",
    "    if domain == main_domain or domain.endswith(f\".{main_domain}\"):\n",
    "        return True\n",
    "\n",
    "    # Allow explicitly allowed additional domains\n",
    "    return any(\n",
    "        domain == add_domain or domain.endswith(f\".{add_domain}\")\n",
    "        for add_domain in additional_domains\n",
    "    )\n",
    "\n",
    "\n",
    "def is_pdf_file(link):\n",
    "    \"\"\"Check if the link points to a PDF file.\"\"\"\n",
    "    return link.lower().endswith(\".pdf\")\n",
    "\n",
    "\n",
    "def download_pdf(url, save_folder):\n",
    "    \"\"\"Download a PDF file from the given URL.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        filename = os.path.join(save_folder, os.path.basename(urlparse(url).path))\n",
    "        with open(filename, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"Downloaded PDF: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download PDF {url}: {e}\")\n",
    "\n",
    "\n",
    "def get_page_title(soup):\n",
    "    \"\"\"Extract the title of the webpage.\"\"\"\n",
    "    title_tag = soup.find(\"title\")\n",
    "    return title_tag.get_text(strip=True) if title_tag else \"No Title\"\n",
    "\n",
    "\n",
    "def url_to_filename(url):\n",
    "    \"\"\"\n",
    "    Convert a URL to a filename-friendly string.\n",
    "    Example: 'https://example.com' -> 'example_com'\n",
    "    \"\"\"\n",
    "    parsed_url = urlparse(url)\n",
    "    domain = parsed_url.netloc  # Extract the domain\n",
    "    return domain.replace(\".\", \"_\").replace(\"www_\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_common_sections(soup):\n",
    "    \"\"\"\n",
    "    Remove common menu, header, and footer sections from the parsed HTML.\n",
    "    \"\"\"\n",
    "    # Define common section selectors to remove\n",
    "    common_selectors = [\"header\", \"footer\", \".menu\", \".navbar\", \".footer\"]  #\n",
    "\n",
    "    for selector in common_selectors:\n",
    "        for element in soup.select(selector):\n",
    "            element.decompose()  # Remove the element from the DOM\n",
    "\n",
    "    return soup\n",
    "\n",
    "\n",
    "def is_relevant_content(text):\n",
    "    \"\"\"\n",
    "    Check if the text is relevant and not boilerplate or empty.\n",
    "    \"\"\"\n",
    "    irrelevant_phrases = [\n",
    "        \"top of the page\",\n",
    "        \"skip to content\",\n",
    "        \"loading...\",\n",
    "        \"menu\",\n",
    "        \"navigation\",\n",
    "    ]\n",
    "    # Check if the text is non-empty and not in the irrelevant phrases\n",
    "    return (\n",
    "        text\n",
    "        and text.strip()\n",
    "        and not any(phrase in text.lower() for phrase in irrelevant_phrases)\n",
    "    )\n",
    "\n",
    "\n",
    "def format_content(soup):\n",
    "    \"\"\"\n",
    "    Format the content of a webpage by preserving headers, bullet points, and numbered lists,\n",
    "    while avoiding duplicate and irrelevant content, and normalizing spaces.\n",
    "    \"\"\"\n",
    "    formatted_content = []\n",
    "    seen_content = set()  # Track already added content to avoid duplicates\n",
    "\n",
    "    for element in soup.find_all(\n",
    "        [\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"p\", \"li\", \"label\", \"span\", \"div\"]\n",
    "    ):\n",
    "        # Skip tags that have child elements already processed\n",
    "        if element.findChildren():\n",
    "            continue\n",
    "\n",
    "        # Extract and normalize text\n",
    "        text = \" \".join(element.stripped_strings)  # Combine child strings with spaces\n",
    "        text = re.sub(\n",
    "            r\"\\s+\", \" \", text\n",
    "        )  # Replace multiple spaces/newlines with a single space\n",
    "        text = text.strip()\n",
    "\n",
    "        # Skip if the text is irrelevant or already seen\n",
    "        if is_relevant_content(text) and text not in seen_content:\n",
    "            # Add headers with line breaks\n",
    "            if element.name.startswith(\"h\"):\n",
    "                formatted_content.append(f\"\\n{text}\\n\")\n",
    "            # Add bullet points for list items\n",
    "            elif element.name == \"li\":\n",
    "                formatted_content.append(f\"- {text}\")\n",
    "            # Add paragraphs or other content as-is\n",
    "            else:\n",
    "                formatted_content.append(text)\n",
    "            seen_content.add(text)  # Mark content as added\n",
    "\n",
    "    # Join all elements with newlines\n",
    "    return \"\\n\".join(formatted_content)\n",
    "\n",
    "\n",
    "def should_ignore_link(url):\n",
    "    \"\"\"\n",
    "    Check if the URL should be ignored based on its extension or specific keywords in the content.\n",
    "    \"\"\"\n",
    "    # File extensions to ignore\n",
    "    ignore_extensions = {\n",
    "        \".jpg\",\n",
    "        \".jpeg\",\n",
    "        \".png\",\n",
    "        \".gif\",\n",
    "        \".bmp\",\n",
    "        \".tiff\",\n",
    "        \".svg\",\n",
    "        \".webp\",\n",
    "        \".mp4\",\n",
    "        \".avi\",\n",
    "        \".mov\",\n",
    "        \".mkv\",\n",
    "        \".flv\",\n",
    "        \".wmv\",\n",
    "        \".webm\",\n",
    "        \".3gp\",\n",
    "        \".mp3\",\n",
    "        \".wav\",\n",
    "        \".ogg\",\n",
    "        \".aac\",\n",
    "        \".flac\",\n",
    "        \".m4a\",\n",
    "        \".pdf\",\n",
    "        \".doc\",\n",
    "        \".docx\",\n",
    "        \".ppt\",\n",
    "        \".pptx\",\n",
    "        \".xls\",\n",
    "        \".xlsx\",\n",
    "        \".rtf\",\n",
    "        \".odt\",\n",
    "        \".ods\",\n",
    "        \".zip\",\n",
    "        \".rar\",\n",
    "        \".7z\",\n",
    "        \".tar\",\n",
    "        \".gz\",\n",
    "        \".bz2\",\n",
    "        \".exe\",\n",
    "        \".bat\",\n",
    "        \".sh\",\n",
    "        \".msi\",\n",
    "        \".bin\",\n",
    "        \".js\",\n",
    "        \".css\",\n",
    "        \".json\",\n",
    "        \".xml\",\n",
    "        \".yaml\",\n",
    "        \".yml\",\n",
    "        \".ico\",\n",
    "        \".eot\",\n",
    "        \".ttf\",\n",
    "        \".woff\",\n",
    "        \".woff2\",\n",
    "        \".swf\",\n",
    "        \".apk\",\n",
    "        \".aspx\",\n",
    "    }\n",
    "\n",
    "    # Keywords to ignore\n",
    "    ignore_keywords = [\n",
    "        r\"\\bnews\\b\",\n",
    "        r\"\\bevents\\b\",\n",
    "        r\"\\bcalendar\\b\",\n",
    "        r\"\\binternship\\b\",\n",
    "        r\"\\bdashboard\\b\",\n",
    "        r\"\\bgiving\\b\",\n",
    "        r\"\\bsheet\\b\",\n",
    "        r\"/search\",\n",
    "        r\"/tag\",\n",
    "        r\"\\bcollection\\b\",\n",
    "        \"wiki\",\n",
    "        \"login\",\n",
    "        r\"\\bdownload\\b\",\n",
    "        r\"\\bpresident\\b\",\n",
    "        r\"\\bbluebook\\b\",\n",
    "        r\"\\blib\\.\\b\",\n",
    "        r\"/lib\",\n",
    "        r\"/documents\",\n",
    "        r\"\\bpublicsafty\\b\",\n",
    "        r\"\\bhistory\\b\",\n",
    "        r\"\\bemployment\\b\",\n",
    "        r\"\\bchapter\\b\",\n",
    "        r\"\\bfaq\\b\",\n",
    "        r\"\\bmilitary\\b\",\n",
    "        r\"\\bfacilities\\b\",\n",
    "        r\"\\bguides\\b\",\n",
    "        r\"\\bcareer\\b\",\n",
    "        r\"\\bbudget\\b\",\n",
    "        r\"\\bmytraining\\b\",\n",
    "        r\"\\btour\\b\",\n",
    "        r\"(^|[^a-zA-Z0-9])undergraduate([^a-zA-Z0-9]|$)\",\n",
    "        r\"\\btoday\\b\",\n",
    "        r\"\\bindex\\.php\\b\",\n",
    "        r\"\\bstudyabroad\\b\",\n",
    "    ]\n",
    "\n",
    "    # Exception keywords to allow\n",
    "    allow_keywords = [r\"\\bprofile\\b\", r\"\\bfaculty\\b\", r\"\\bgraduate\\b\"]\n",
    "\n",
    "    # Check for ignored extensions\n",
    "    if any(url.lower().endswith(ext) for ext in ignore_extensions):\n",
    "        return True  # Ignore if the URL has a disallowed extension\n",
    "\n",
    "    # Check if the URL contains any of the allow keywords\n",
    "    if any(re.search(allow, url.lower()) for allow in allow_keywords):\n",
    "        return False  # Do not ignore if an allow keyword is found\n",
    "\n",
    "    # Check if the URL contains any of the ignore keywords\n",
    "    if any(re.search(ignore, url.lower()) for ignore in ignore_keywords):\n",
    "        return True  # Ignore if an ignore keyword is found\n",
    "\n",
    "    # Check for YEAR pattern (e.g., 20{00-30})\n",
    "    if re.search(r\"\\b20(0[0-9]|1[0-9]|2[0-9]|30)\\b\", url):\n",
    "        return True  # Ignore if a YEAR pattern is found\n",
    "\n",
    "    return False  # Do not ignore otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_website(base_url, additional_domains, max_links=None, save_interval=1000):\n",
    "    \"\"\"\n",
    "    Crawl a website and extract all text content, saving the output every `save_interval` pages.\n",
    "    \"\"\"\n",
    "    # Parse the main domain from the base URL\n",
    "    # parsed_base = urlparse(base_url)\n",
    "    # main_domain = parsed_base.netloc\n",
    "    main_domain = get_main_domain(base_url)\n",
    "\n",
    "    visited = set()\n",
    "    to_visit = [base_url]\n",
    "    output_text = \"\"\n",
    "    saved_files_count = 0  # Counter for saved files\n",
    "\n",
    "    # Create folder for PDFs\n",
    "    pdf_folder = f\"pdfs/{url_to_filename(base_url)}\"\n",
    "    os.makedirs(pdf_folder, exist_ok=True)\n",
    "\n",
    "    crawled_count = 0  # Counter for the number of links processed\n",
    "\n",
    "    while to_visit:\n",
    "        if max_links is not None and crawled_count >= max_links:\n",
    "            print(f\"Reached the test limit of {max_links} links.\")\n",
    "            break\n",
    "\n",
    "        current_url = to_visit.pop(0)\n",
    "        if current_url in visited:\n",
    "            continue\n",
    "        if should_ignore_link(current_url):\n",
    "            print(f\"Link contains ignore list keywords: {current_url}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"{crawled_count}: Visiting: {current_url}\")\n",
    "        visited.add(current_url)\n",
    "        crawled_count += 1\n",
    "\n",
    "        if is_pdf_file(current_url):\n",
    "            print(\"PDF found but not downloaded.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            response = requests.get(current_url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            soup_base = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # Remove common sections\n",
    "            soup = remove_common_sections(soup_base)\n",
    "\n",
    "            # Extract title and formatted content\n",
    "            title = get_page_title(soup)\n",
    "            content = format_content(soup)\n",
    "\n",
    "            # Append to output in the specified format\n",
    "            output_text += (\n",
    "                f\"LINK: {current_url}\\nTITLE: {title}\\nCONTENT:\\n{content}\\n\\n\"\n",
    "            )\n",
    "\n",
    "            # Save the content every `save_interval` pages\n",
    "            if crawled_count % save_interval == 0:\n",
    "                save_file(output_text, base_url, crawled_count)\n",
    "                saved_files_count += 1\n",
    "                output_text = \"\"  # Reset the output content after saving\n",
    "\n",
    "            # Find all links on the page\n",
    "            for link in soup_base.find_all(\"a\", href=True):\n",
    "                current_base = response.url  # Use the actual URL of the current page\n",
    "                full_url = urljoin(current_base, link[\"href\"])\n",
    "\n",
    "                # Remove fragment identifiers from the URL\n",
    "                parsed_url = urlparse(full_url)\n",
    "                full_url = urlunparse(parsed_url._replace(fragment=\"\"))\n",
    "                if (\n",
    "                    is_allowed_domain(full_url, main_domain, additional_domains)\n",
    "                    and full_url not in visited\n",
    "                ):\n",
    "                    to_visit.append(full_url)\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"Request timed out for URL: {current_url}\")\n",
    "            continue\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed for URL: {current_url} - {e}\")\n",
    "            continue\n",
    "\n",
    "    # Save any remaining content if the crawl ends\n",
    "    if output_text.strip():\n",
    "        print(\"5 step\")\n",
    "        save_file(output_text, base_url, crawled_count)\n",
    "\n",
    "    print(f\"Total saved files: {saved_files_count}\")\n",
    "    return output_text\n",
    "\n",
    "\n",
    "def save_file(content, base_url, page_count):\n",
    "    \"\"\"\n",
    "    Save the parsed content to a file with the page count in the filename.\n",
    "    \"\"\"\n",
    "    filename = url_to_filename(base_url)\n",
    "    os.makedirs(\"../data/crawled_content\", exist_ok=True)  # Ensure the directory exists\n",
    "    file_path = f\"../data/crawled_content/{filename}_{page_count}.txt\"\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(content)\n",
    "    print(f\"Saved content to: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Visiting: https://utexas.edu/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_120108/835610855.py:46: DeprecationWarning: Call to deprecated method findChildren. (Replaced by find_all) -- Deprecated since version 3.0.0.\n",
      "  if element.findChildren():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: Visiting: https://www.utexas.edu/\n",
      "Link contains ignore list keywords: https://giving.utexas.edu/what-starts-here?utm_source=main&amp;utm_medium=test&amp;utm_campaign=launch\n",
      "Link contains ignore list keywords: https://news.utexas.edu/2025/05/12/celebrating-the-class-of-2025/\n",
      "Link contains ignore list keywords: https://news.utexas.edu/2025/05/09/making-ut-shine-for-commencement/\n",
      "Link contains ignore list keywords: https://news.utexas.edu/2025/05/05/ready-to-change-the-world-dat-duong/\n",
      "Link contains ignore list keywords: https://www.news.utexas.edu/\n",
      "Link contains ignore list keywords: https://calendar.utexas.edu/\n",
      "2: Visiting: https://admissions.utexas.edu/\n",
      "3: Visiting: https://faculty.utexas.edu/career\n",
      "Link contains ignore list keywords: https://president.utexas.edu/innovation-board\n",
      "Link contains ignore list keywords: https://giving.utexas.edu/foundation-relations/\n",
      "Link contains ignore list keywords: https://www.utexas.edu/military\n",
      "4: Visiting: https://discoveries.utexas.edu/about/\n",
      "Reached the test limit of 5 links.\n",
      "5 step\n",
      "Saved content to: ../data/crawled_content/utexas_edu_5.txt\n",
      "Total saved files: 0\n",
      "Crawling complete.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Set `max_links` for testing or `None` for production.\n",
    "    max_test_links = 5\n",
    "\n",
    "    website_url_ls = [\n",
    "        \"https://utexas.edu/\",\n",
    "    ]  # \"https://gradschool.utexas.edu/degrees-programs\"\n",
    "\n",
    "    for website_url in website_url_ls:\n",
    "        filename = url_to_filename(website_url)\n",
    "        additional_domains = {\"relateddomain.com\"}\n",
    "\n",
    "        # Crawl with a save interval of 1000 pages\n",
    "        result = crawl_website(\n",
    "            website_url,\n",
    "            additional_domains,\n",
    "            max_links=max_test_links,\n",
    "            save_interval=2000,\n",
    "        )\n",
    "        print(\"Crawling complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Set `max_links` for testing or `None` for production.\n",
    "    max_test_links = 10000\n",
    "\n",
    "    website_url_ls = [\n",
    "        \"https://www.tamu.edu/\"\n",
    "    ]  # \"https://grad.tamu.edu/academics/program-directory\", \"https://www.tamu.edu/academics/colleges-schools/index.html\" ,\n",
    "\n",
    "    for website_url in website_url_ls:\n",
    "        filename = url_to_filename(website_url)\n",
    "        additional_domains = {\"relateddomain.com\"}\n",
    "\n",
    "        # Crawl with a save interval of 1000 pages\n",
    "        result = crawl_website(\n",
    "            website_url,\n",
    "            additional_domains,\n",
    "            max_links=max_test_links,\n",
    "            save_interval=2000,\n",
    "        )\n",
    "        print(\"Crawling complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Set `max_links` for testing or `None` for production.\n",
    "    max_test_links = 10000\n",
    "\n",
    "    website_url = \"https://www.utdallas.edu/\"\n",
    "    filename = url_to_filename(website_url)\n",
    "    additional_domains = {\"relateddomain.com\"}\n",
    "\n",
    "    # Crawl with a save interval of 1000 pages\n",
    "    result = crawl_website(\n",
    "        website_url, additional_domains, max_links=max_test_links, save_interval=2000\n",
    "    )\n",
    "    print(\"Crawling complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genu-dGEUhqWF-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
